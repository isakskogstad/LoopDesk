version: "3.8"

services:
  # RSSHub - RSS feed generator for social media
  rsshub:
    image: diygod/rsshub:latest
    restart: unless-stopped
    ports:
      - "1200:1200"
    environment:
      NODE_ENV: production
      CACHE_TYPE: redis
      CACHE_EXPIRE: 300
      REDIS_URL: "redis://redis:6379/"
      # Rate limiting
      REQUEST_RETRY: 2
      REQUEST_TIMEOUT: 30000
      # Access control (optional - set in .env)
      ACCESS_KEY: ${RSSHUB_ACCESS_KEY:-}
      # Twitter/X credentials (optional)
      TWITTER_USERNAME: ${TWITTER_USERNAME:-}
      TWITTER_PASSWORD: ${TWITTER_PASSWORD:-}
      TWITTER_AUTH_TOKEN: ${TWITTER_AUTH_TOKEN:-}
      # Instagram credentials (optional)
      IG_USERNAME: ${IG_USERNAME:-}
      IG_PASSWORD: ${IG_PASSWORD:-}
      IG_PROXY: ${IG_PROXY:-}
      # YouTube API (optional)
      YOUTUBE_KEY: ${YOUTUBE_KEY:-}
      # GitHub token (optional)
      GITHUB_ACCESS_TOKEN: ${GITHUB_ACCESS_TOKEN:-}
      # Telegram (optional)
      TELEGRAM_TOKEN: ${TELEGRAM_TOKEN:-}
      # Puppeteer for JavaScript-rendered pages
      PUPPETEER_WS_ENDPOINT: "ws://browserless:3000"
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:1200/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Redis for RSSHub caching
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 5s

  # Browserless for Puppeteer (headless Chrome)
  # Required for some routes that need JavaScript rendering
  browserless:
    image: browserless/chrome:latest
    restart: unless-stopped
    environment:
      MAX_CONCURRENT_SESSIONS: 5
      CONNECTION_TIMEOUT: 60000
      MAX_QUEUE_LENGTH: 10
    ulimits:
      core:
        hard: 0
        soft: 0
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/pressure"]
      interval: 30s
      timeout: 10s
      retries: 3

  # =============================================
  # SOCIAL MEDIA SCRAPERS
  # =============================================

  # LinkedIn MCP Server - Scrapes LinkedIn profiles and companies
  # Requires li_at cookie from logged-in LinkedIn session
  linkedin-scraper:
    image: stickerdaniel/linkedin-mcp-server:latest
    restart: unless-stopped
    ports:
      - "8100:8000"
    environment:
      LINKEDIN_COOKIE: ${LINKEDIN_COOKIE:-}
    command: ["--transport", "streamable-http", "--host", "0.0.0.0", "--port", "8000", "--path", "/mcp"]
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8000/mcp"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s

  # Python Social Scraper Service - Twitter (twscrape) + Facebook
  social-scraper:
    build:
      context: ./services/social-scraper
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "8101:8000"
    environment:
      # Twitter accounts (JSON format: [{"username": "...", "password": "...", "email": "...", "cookies": "..."}])
      TWITTER_ACCOUNTS: ${TWITTER_ACCOUNTS:-}
      # Facebook cookies (for authenticated scraping)
      FACEBOOK_COOKIES: ${FACEBOOK_COOKIES:-}
    volumes:
      - social-scraper-data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

volumes:
  redis-data:
  social-scraper-data:
